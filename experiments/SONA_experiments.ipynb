{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HJ7L0CvLzux"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOy0DXenLIrQ"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn smote-variants optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2egSKw7LP7J"
      },
      "outputs": [],
      "source": [
        "import imblearn.datasets as imb_datasets\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import inspect\n",
        "import smote_variants\n",
        "\n",
        "# Define data list\n",
        "data_List = [\n",
        "        \"ecoli\",\n",
        "        \"satimage\",\n",
        "        \"pen_digits\",\n",
        "        \"sick_euthyroid\",\n",
        "        \"libras_move\",\n",
        "        \"car_eval_4\",\n",
        "        \"wine_quality\",\n",
        "        \"ozone_level\",\n",
        "        \"mammography\",\n",
        "        \"optical_digits\",\n",
        "]\n",
        "\n",
        "# Load and prepare data\n",
        "train_test_List = []\n",
        "for data_name in data_List:\n",
        "    try:\n",
        "        libras = imb_datasets.fetch_datasets()[data_name]\n",
        "        X, y = libras['data'], libras['target']\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "        # Rename class -1 to 0 if present\n",
        "        y_train = np.where(y_train == -1, 0, y_train)\n",
        "        y_test = np.where(y_test == -1, 0, y_test)\n",
        "\n",
        "        train_test_List.append((data_name, (X_train, X_test, y_train, y_test)))\n",
        "        print(f\"Loaded {data_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load {data_name}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P838yeSbMNsQ"
      },
      "source": [
        "# Oversampling methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uoVJwUTpLgkp"
      },
      "outputs": [],
      "source": [
        "oversampling_methods = [\n",
        "    ('SONA',{}),\n",
        "    None,  # Original data\n",
        "    ('SMOTE', {}),\n",
        "    ('Borderline_SMOTE2', {}),\n",
        "    ('Safe_Level_SMOTE', {}),\n",
        "    ('polynom_fit_SMOTE_poly', {}),\n",
        "    ('SMOTE_IPF', {}),\n",
        "]\n",
        "classifiers = [\n",
        "    ('Logistic regression', {}),\n",
        "    ('XGBoost', {}),\n",
        "     ('KNeighborsClassifier', {}),\n",
        "    ('DecisionTreeClassifier', {}),\n",
        "    ('MLPClassifier', {}),\n",
        "    ('SVC', {}),\n",
        "    ('RandomForestClassifier', {}),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lBio1h1lL_0I"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def SONA(X, y, min_label, new_label=0):\n",
        "    X_gen_min = X[y == min_label]\n",
        "    X_gen_maj = X[y != min_label]\n",
        "\n",
        "    maj_size = len(X_gen_maj)\n",
        "    minor_size = len(X_gen_min)\n",
        "    synthese_len = maj_size - minor_size\n",
        "\n",
        "    if synthese_len <= 0:\n",
        "        return X, y\n",
        "\n",
        "    # 1. Distance Matrices\n",
        "    dist_min2maj = cdist(X_gen_min, X_gen_maj)\n",
        "    \n",
        "    # 2. Identify Borders\n",
        "    closest_min_idx = np.argmin(dist_min2maj, axis=0)\n",
        "    neg_border = np.bincount(closest_min_idx, minlength=minor_size)\n",
        "\n",
        "    # Find index of closest majority for each minority (Positive Border)\n",
        "    closest_maj_idx = np.argmin(dist_min2maj, axis=1)\n",
        "    pos_border = np.bincount(closest_maj_idx, minlength=maj_size)\n",
        "\n",
        "\n",
        "    # 3. Calculate Radius\n",
        "    safe_maj_mask = (pos_border == 0)\n",
        "    if not np.any(safe_maj_mask):\n",
        "        neg_radius = np.min(dist_min2maj, axis=1)\n",
        "    else:\n",
        "        neg_radius = np.min(dist_min2maj[:, safe_maj_mask], axis=1)\n",
        "\n",
        "    # 4. Sampling Probabilities\n",
        "    prop_min = 1.0 / (neg_border + 1.0)\n",
        "    prop_min /= prop_min.sum()\n",
        "\n",
        "    # 5. Generate Synthetic Samples\n",
        "    idx_i = np.random.choice(minor_size, size=synthese_len, p=prop_min)\n",
        "    X_i = X_gen_min[idx_i]\n",
        "    \n",
        "    # Pick neighbor points (j) for each i\n",
        "    dist_min2min = cdist(X_gen_min, X_gen_min)\n",
        "    \n",
        "    # Apply inverse distance weighting for neighbors\n",
        "    with np.errstate(divide='ignore'):\n",
        "        terminal_weights = 1.0 / dist_min2min\n",
        "    np.fill_diagonal(terminal_weights, 0) # Can't pick itself\n",
        "    terminal_weights = np.nan_to_num(terminal_weights, posinf=0)\n",
        "    \n",
        "    # Normalize each row to be a probability distribution\n",
        "    row_sums = terminal_weights.sum(axis=1, keepdims=True)\n",
        "    terminal_probs = terminal_weights / row_sums\n",
        "\n",
        "    # For each selected i, pick a neighbor j\n",
        "    idx_j = [np.random.choice(minor_size, p=terminal_probs[i]) for i in idx_i]\n",
        "    X_j = X_gen_min[idx_j]\n",
        "\n",
        "    # 6. Direction and Interpolation\n",
        "    diff = X_j - X_i\n",
        "    norm_v = np.linalg.norm(diff, axis=1, keepdims=True)\n",
        "    # Avoid division by zero for identical points\n",
        "    norm_v_safe = np.where(norm_v == 0, 1, norm_v)\n",
        "    \n",
        "    direction_vector = diff / norm_v_safe\n",
        "    alpha = np.random.random((synthese_len, 1))\n",
        "    \n",
        "    # Determine step size: min(radius of point i, distance to point j)\n",
        "    step_size = np.minimum(neg_radius[idx_i].reshape(-1, 1), norm_v)\n",
        "    \n",
        "    syn_samples = X_i + (alpha * direction_vector * step_size)\n",
        "\n",
        "    return (\n",
        "        np.vstack([X, syn_samples]),\n",
        "        np.hstack([y, np.repeat(min_label + new_label, synthese_len)]),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D4kstOzLMeCQ"
      },
      "outputs": [],
      "source": [
        "def oversampling_only(oversampler_config, X_train, X_test, y_train, y_test):\n",
        "      X_train_resampled, y_train_resampled = X_train, y_train\n",
        "      if oversampler_config != None:\n",
        "          oversampler_class_name, oversampler_params = oversampler_config\n",
        "\n",
        "          # Use the specific oversampler directly\n",
        "          if oversampler_class_name == \"SONA\":\n",
        "              X_train_resampled, y_train_resampled = SONA(X_train, y_train, 1)\n",
        "          else:\n",
        "            oversampler_instance = getattr(sv, oversampler_class_name)(**oversampler_params)\n",
        "            X_train_resampled, y_train_resampled = oversampler_instance.sample(X_train, y_train)\n",
        "\n",
        "      # print(\"return \",oversampler_config)\n",
        "\n",
        "      return X_train_resampled, y_train_resampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI9AfmV1N9o_"
      },
      "source": [
        "# Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PYmSIj_tOjA_"
      },
      "outputs": [],
      "source": [
        "import imblearn.datasets as imb_datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import smote_variants as sv\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def get_classifier_params(trial, classifier_name):\n",
        "    \"\"\"Defines search spaces for classifiers.\"\"\"\n",
        "    if classifier_name == 'Logistic regression':\n",
        "        return {\n",
        "            'C': trial.suggest_float('C', 1e-4, 1e2, log=True),\n",
        "            'solver': trial.suggest_categorical('solver', ['liblinear', 'lbfgs'])\n",
        "        }\n",
        "    elif classifier_name == 'XGBoost':\n",
        "        return {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'verbosity': 0\n",
        "        }\n",
        "    elif classifier_name == 'KNeighborsClassifier':\n",
        "        return {\n",
        "            'n_neighbors': trial.suggest_int('n_neighbors', 3, 20),\n",
        "            'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
        "            'metric': trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski'])\n",
        "        }\n",
        "    elif classifier_name == 'DecisionTreeClassifier':\n",
        "        return {\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
        "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "            'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
        "        }\n",
        "    elif classifier_name == 'MLPClassifier':\n",
        "        return {\n",
        "            'hidden_layer_sizes': trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (50, 50)]),\n",
        "            'activation': trial.suggest_categorical('activation', ['tanh', 'relu']),\n",
        "            'alpha': trial.suggest_float('alpha', 1e-4, 1e-1, log=True),\n",
        "            'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-3, 1e-1, log=True),\n",
        "            'max_iter': 500\n",
        "        }\n",
        "    elif classifier_name == 'SVC':\n",
        "        return {\n",
        "            'C': trial.suggest_float('C', 0.1, 100, log=True),\n",
        "            'kernel': trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly']),\n",
        "            'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
        "            'probability': True,\n",
        "        }\n",
        "    elif classifier_name == 'RandomForestClassifier':\n",
        "        return {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
        "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20)\n",
        "        }\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown classifier: {classifier_name}\")\n",
        "\n",
        "def get_classifier_instance(classifier_name, params):\n",
        "    if classifier_name == 'Logistic regression':\n",
        "        return LogisticRegression(**params)\n",
        "    elif classifier_name == 'XGBoost':\n",
        "        return XGBClassifier(**params)\n",
        "    elif classifier_name == 'KNeighborsClassifier':\n",
        "        return KNeighborsClassifier(**params)\n",
        "    elif classifier_name == 'DecisionTreeClassifier':\n",
        "        return DecisionTreeClassifier(**params)\n",
        "    elif classifier_name == 'MLPClassifier':\n",
        "        return MLPClassifier(**params)\n",
        "    elif classifier_name == 'SVC':\n",
        "        return SVC(**params)\n",
        "    elif classifier_name == 'RandomForestClassifier':\n",
        "        return RandomForestClassifier(**params)\n",
        "    elif classifier_name == 'GaussianNB':\n",
        "        return GaussianNB(**params)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown classifier: {classifier_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPsbEGOQOADv"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial, classifier_name, X_train, y_train):\n",
        "    # Retrieve hyperparameters for the current classifier using the Optuna trial\n",
        "    params = get_classifier_params(trial, classifier_name)\n",
        "\n",
        "    # Instantiate the classifier with the suggested hyperparameters\n",
        "    classifier = get_classifier_instance(classifier_name, params)\n",
        "\n",
        "    # Create a pipeline with StandardScaler and the classifier\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', classifier)\n",
        "    ])\n",
        "\n",
        "    # Initialize StratifiedKFold for cross-validation\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Evaluate the pipeline using cross_val_score with weighted F1-score\n",
        "    scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='f1_weighted', n_jobs=-1)\n",
        "\n",
        "    # Return the mean F1-score from cross-validation\n",
        "    return scores.mean()\n",
        "\n",
        "print(\"Objective function 'objective' defined successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8aBbmR9OR3W"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for data_name, (X_train, X_test, y_train, y_test) in train_test_List:\n",
        "    print(f\"\\nProcessing dataset: {data_name}\")\n",
        "    for classifier_name, _ in classifiers:\n",
        "        print(f\"  Optimizing for classifier: {classifier_name}\")\n",
        "\n",
        "        # Create an Optuna study for each classifier and dataset\n",
        "        study = optuna.create_study(direction='maximize')\n",
        "        study.optimize(lambda trial: objective(trial, classifier_name, X_train, y_train), n_trials=10, show_progress_bar=True)\n",
        "\n",
        "        # Store the best results\n",
        "        results.append({\n",
        "            'dataset': data_name,\n",
        "            'classifier': classifier_name,\n",
        "            'best_f1_score': study.best_value,\n",
        "            'best_params': study.best_params\n",
        "        })\n",
        "\n",
        "# Convert results to a Pandas DataFrame for summary\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"best_param.csv\")\n",
        "\n",
        "print(\"Hyperparameter optimization complete. Results stored in 'results_df'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iExAFL2rMx5w"
      },
      "source": [
        "# Training & Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5AwW2QBNHCo"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "\n",
        "results_df = pd.read_csv(\"best_param.csv\") # getfrom optuna\n",
        "param_dict = results_df.set_index(['dataset', 'classifier'])['best_params'].to_dict()\n",
        "\n",
        "evaluation_results = [] # Re-initialize the evaluation_results list\n",
        "\n",
        "for dataset_name, data_tuple in train_test_List:\n",
        "    X_train, X_test, y_train, y_test = data_tuple\n",
        "    print(\"dataset_name:\", dataset_name)\n",
        "\n",
        "    for oversampler_config in oversampling_methods:\n",
        "      if oversampler_config != None:\n",
        "        oversampler_class_name, oversampler_params = oversampler_config\n",
        "      else:\n",
        "        oversampler_class_name = \"None\"\n",
        "        oversampler_params = {}\n",
        "\n",
        "      print(\"\\t oversampler_config:\", oversampler_config)\n",
        "\n",
        "      for classifier_name, _ in classifiers:\n",
        "        print(\"\\t\\t classifier_name:\", classifier_name)\n",
        "\n",
        "        # Instantiate classifier and create pipeline\n",
        "        best_params_str = param_dict[(dataset_name, classifier_name)]\n",
        "        # Convert the string representation of the dictionary back to a dictionary\n",
        "        best_params = ast.literal_eval(best_params_str)\n",
        "\n",
        "        accuracy_list = []\n",
        "        precision_list = []\n",
        "        recall_list = []\n",
        "        f1_list = []\n",
        "        roc_auc_list = []\n",
        "\n",
        "        # repeat for reproducibility\n",
        "        for _ in range(7):\n",
        "          X_train2, y_train2 = oversampling_only(oversampler_config, X_train, X_test, y_train, y_test)\n",
        "\n",
        "          classifier = get_classifier_instance(classifier_name, best_params)\n",
        "          pipeline = Pipeline(\n",
        "              [('scaler', StandardScaler()), ('classifier', classifier)]\n",
        "          )\n",
        "\n",
        "          # Train the pipeline\n",
        "          pipeline.fit(X_train2, y_train2)\n",
        "\n",
        "          # Make predictions\n",
        "          y_pred = pipeline.predict(X_test)\n",
        "\n",
        "          # Calculate metrics\n",
        "          accuracy = accuracy_score(y_test, y_pred)\n",
        "          precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "          recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "          f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "          roc_auc = np.nan # Initialize as NaN\n",
        "          # Try to get probability predictions for ROC AUC\n",
        "          if hasattr(pipeline.named_steps['classifier'], 'predict_proba'):\n",
        "              y_proba = pipeline.predict_proba(X_test)\n",
        "              unique_classes = np.unique(y_test)\n",
        "\n",
        "              if len(unique_classes) > 1:\n",
        "                  if len(unique_classes) == 2: # Binary classification\n",
        "\n",
        "                      pos_class_label = unique_classes[-1]\n",
        "                      # Get the order of classes learned by the classifier\n",
        "                      class_labels_in_model = pipeline.named_steps['classifier'].classes_\n",
        "                      # Find the index corresponding to the positive class label\n",
        "                      pos_class_idx = np.where(class_labels_in_model == pos_class_label)[0][0]\n",
        "\n",
        "                      y_score_for_roc = y_proba[:, pos_class_idx]\n",
        "\n",
        "                      try:\n",
        "                          # For binary classification, 'multi_class' is not needed, and y_score must be 1D.\n",
        "                          roc_auc = roc_auc_score(y_test, y_score_for_roc, average='weighted')\n",
        "                      except ValueError as e:\n",
        "                          print(f\"Warning: Could not compute ROC AUC for binary case {data_name} with {classifier_name}: {e}\")\n",
        "\n",
        "                  else: # Multi-class classification (len(unique_classes) > 2)\n",
        "                      # For multi-class classification, roc_auc_score expects a 2D array (n_samples, n_classes).\n",
        "                      # The columns of y_proba should correspond to the order of classes in unique_classes.\n",
        "                      if y_proba.ndim == 2 and y_proba.shape[1] == len(unique_classes):\n",
        "                          try:\n",
        "                              roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted', labels=unique_classes)\n",
        "                          except ValueError as e:\n",
        "                              print(f\"Warning: Could not compute ROC AUC for multi-class case {data_name} with {classifier_name}: {e}\")\n",
        "                      else:\n",
        "                          print(f\"Warning: Unexpected y_proba shape for multi-class classification in {data_name} with {classifier_name}. ROC AUC will be NaN.\")\n",
        "              else:\n",
        "                  print(f\"Warning: Cannot compute ROC AUC for {data_name} with classifier {classifier_name} as y_test contains only one class.\")\n",
        "          else:\n",
        "              print(f\"Warning: Classifier {classifier_name} does not support predict_proba. ROC AUC will be NaN.\")\n",
        "\n",
        "          accuracy_list.append(accuracy)\n",
        "          precision_list.append(precision)\n",
        "          recall_list.append(recall)\n",
        "          f1_list.append(f1)\n",
        "          roc_auc_list.append(roc_auc)\n",
        "\n",
        "        # Store results\n",
        "        evaluation_results.append({\n",
        "            'dataset': dataset_name,\n",
        "            'classifier': classifier_name,\n",
        "            'oversamping': oversampler_class_name,\n",
        "            'accuracy': accuracy_list,\n",
        "            'precision': precision_list,\n",
        "            'recall': recall_list,\n",
        "            'f1_score': f1_list,\n",
        "            'roc_auc': roc_auc_list,\n",
        "        })\n",
        "\n",
        "print(\"Model evaluation complete. Results stored in 'evaluation_results'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWwDRY5INY4x"
      },
      "outputs": [],
      "source": [
        "evaluation_df = pd.DataFrame(evaluation_results)\n",
        "print(\"Evaluation DataFrame created successfully:\")\n",
        "display(evaluation_df)\n",
        "evaluation_df.to_csv(\"SONA_results.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
