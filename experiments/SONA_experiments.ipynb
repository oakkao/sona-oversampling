{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset preparation"
      ],
      "metadata": {
        "id": "2HJ7L0CvLzux"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOy0DXenLIrQ"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn smote-variants optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imblearn.datasets as imb_datasets\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import inspect\n",
        "import smote_variants\n",
        "\n",
        "# Define data list\n",
        "data_List = [\n",
        "        \"ecoli\",\n",
        "        \"satimage\",\n",
        "        \"pen_digits\",\n",
        "        \"sick_euthyroid\",\n",
        "        \"libras_move\",\n",
        "        \"car_eval_4\",\n",
        "        \"wine_quality\",\n",
        "        \"ozone_level\",\n",
        "        \"mammography\",\n",
        "        \"optical_digits\",\n",
        "]\n",
        "\n",
        "# Load and prepare data\n",
        "train_test_List = []\n",
        "for data_name in data_List:\n",
        "    try:\n",
        "        libras = imb_datasets.fetch_datasets()[data_name]\n",
        "        X, y = libras['data'], libras['target']\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "        # Rename class -1 to 0 if present\n",
        "        y_train = np.where(y_train == -1, 0, y_train)\n",
        "        y_test = np.where(y_test == -1, 0, y_test)\n",
        "\n",
        "        train_test_List.append((data_name, (X_train, X_test, y_train, y_test)))\n",
        "        print(f\"Loaded {data_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load {data_name}: {e}\")"
      ],
      "metadata": {
        "id": "U2egSKw7LP7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Oversampling methods"
      ],
      "metadata": {
        "id": "P838yeSbMNsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oversampling_methods = [\n",
        "    ('SONA',{}),\n",
        "    None,  # Original data\n",
        "    ('SMOTE', {}),\n",
        "    ('Borderline_SMOTE2', {}),\n",
        "    ('Safe_Level_SMOTE', {}),\n",
        "    ('polynom_fit_SMOTE_poly', {}),\n",
        "    ('SMOTE_IPF', {}),\n",
        "]\n",
        "classifiers = [\n",
        "    ('Logistic regression', {}),\n",
        "    ('XGBoost', {}),\n",
        "     ('KNeighborsClassifier', {}),\n",
        "    ('DecisionTreeClassifier', {}),\n",
        "    ('MLPClassifier', {}),\n",
        "    ('SVC', {}),\n",
        "    ('RandomForestClassifier', {}),\n",
        "]"
      ],
      "metadata": {
        "id": "uoVJwUTpLgkp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def SONA(X,y, min_label, new_label = 0):\n",
        "  X_gen_min = (X)[y== min_label]\n",
        "  X_gen_maj = (X)[y != min_label]\n",
        "\n",
        "  maj_size = len(X_gen_maj)\n",
        "  minor_size = len(X_gen_min)\n",
        "\n",
        "  ## Negative border\n",
        "  dist_gen_maj2min= cdist(X_gen_maj, X_gen_min)\n",
        "  rank_dist_maj2min = np.argsort(dist_gen_maj2min)\n",
        "\n",
        "  neg_border = np.zeros(len(X_gen_min))\n",
        "  for i in range(maj_size):\n",
        "    near_point = rank_dist_maj2min[i][0]\n",
        "    neg_border[near_point] += 1\n",
        "\n",
        "  ## Positive border\n",
        "  dist_gen_min2maj = cdist(X_gen_min,X_gen_maj)\n",
        "  rank_dist_min2maj = np.argsort(dist_gen_min2maj)\n",
        "\n",
        "  pos_border = np.zeros(len(X_gen_maj))\n",
        "  for i in range(minor_size):\n",
        "    near_point = rank_dist_min2maj[i][0]\n",
        "    pos_border[near_point] += 1\n",
        "\n",
        "  ## Find radius\n",
        "  neg_radius = np.zeros(len(X_gen_min))\n",
        "  rank_dist_gen_pos = np.argsort(dist_gen_min2maj)\n",
        "\n",
        "  for i in range(minor_size):\n",
        "    pos_list = rank_dist_gen_pos[i]\n",
        "\n",
        "    for j in range(maj_size):\n",
        "      pos_point = pos_list[j]\n",
        "\n",
        "      if pos_border[pos_point] == 0:\n",
        "        neg_radius[i] = dist_gen_min2maj[i][pos_point]\n",
        "        break;\n",
        "\n",
        "  prop_min = 1 / (neg_border+1)\n",
        "  prop_min = prop_min / np.sum(prop_min)\n",
        "\n",
        "  syn_list = []\n",
        "\n",
        "  dist_min2min = cdist(X_gen_min,X_gen_min)\n",
        "\n",
        "  synthese_len = maj_size - minor_size\n",
        "\n",
        "  for t in range(synthese_len):      # assume to 1:1\n",
        "    i = np.random.choice(len(X_gen_min), p = prop_min)\n",
        "\n",
        "    terminal_prop = 1/dist_min2min[i]\n",
        "    terminal_prop = np.nan_to_num(terminal_prop, nan=0, posinf=0, neginf=0)\n",
        "    terminal_prop = terminal_prop / np.sum(terminal_prop)\n",
        "    j = np.random.choice(len(X_gen_min), p = terminal_prop)\n",
        "\n",
        "    direction_vector =  X_gen_min[j] - X_gen_min[i]\n",
        "    norm_v = np.linalg.norm(direction_vector)\n",
        "    direction_vector = direction_vector / norm_v\n",
        "\n",
        "    alpha = np.random.random()\n",
        "\n",
        "    syn_x = X_gen_min[i] + alpha* direction_vector *min(neg_radius[i], norm_v)\n",
        "    syn_list.append(syn_x)\n",
        "\n",
        "  return (\n",
        "            np.vstack([X, syn_list]),\n",
        "            np.hstack([y, np.repeat(min_label + new_label, len(syn_list))]),\n",
        "        )"
      ],
      "metadata": {
        "id": "lBio1h1lL_0I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def oversampling_only(oversampler_config, X_train, X_test, y_train, y_test):\n",
        "      X_train_resampled, y_train_resampled = X_train, y_train\n",
        "      if oversampler_config != None:\n",
        "          oversampler_class_name, oversampler_params = oversampler_config\n",
        "\n",
        "          # Use the specific oversampler directly\n",
        "          if oversampler_class_name == \"SONA\":\n",
        "              X_train_resampled, y_train_resampled = SONA(X_train, y_train, 1)\n",
        "          else:\n",
        "            oversampler_instance = getattr(sv, oversampler_class_name)(**oversampler_params)\n",
        "            X_train_resampled, y_train_resampled = oversampler_instance.sample(X_train, y_train)\n",
        "\n",
        "      # print(\"return \",oversampler_config)\n",
        "\n",
        "      return X_train_resampled, y_train_resampled"
      ],
      "metadata": {
        "id": "D4kstOzLMeCQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter tuning"
      ],
      "metadata": {
        "id": "JI9AfmV1N9o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import imblearn.datasets as imb_datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import smote_variants as sv\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "def get_classifier_params(trial, classifier_name):\n",
        "    \"\"\"Defines search spaces for classifiers.\"\"\"\n",
        "    if classifier_name == 'Logistic regression':\n",
        "        return {\n",
        "            'C': trial.suggest_float('C', 1e-4, 1e2, log=True),\n",
        "            'solver': trial.suggest_categorical('solver', ['liblinear', 'lbfgs'])\n",
        "        }\n",
        "    elif classifier_name == 'XGBoost':\n",
        "        return {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'verbosity': 0\n",
        "        }\n",
        "    elif classifier_name == 'KNeighborsClassifier':\n",
        "        return {\n",
        "            'n_neighbors': trial.suggest_int('n_neighbors', 3, 20),\n",
        "            'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
        "            'metric': trial.suggest_categorical('metric', ['euclidean', 'manhattan', 'minkowski'])\n",
        "        }\n",
        "    elif classifier_name == 'DecisionTreeClassifier':\n",
        "        return {\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
        "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
        "            'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
        "        }\n",
        "    elif classifier_name == 'MLPClassifier':\n",
        "        return {\n",
        "            'hidden_layer_sizes': trial.suggest_categorical('hidden_layer_sizes', [(50,), (100,), (50, 50)]),\n",
        "            'activation': trial.suggest_categorical('activation', ['tanh', 'relu']),\n",
        "            'alpha': trial.suggest_float('alpha', 1e-4, 1e-1, log=True),\n",
        "            'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-3, 1e-1, log=True),\n",
        "            'max_iter': 500\n",
        "        }\n",
        "    elif classifier_name == 'SVC':\n",
        "        return {\n",
        "            'C': trial.suggest_float('C', 0.1, 100, log=True),\n",
        "            'kernel': trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly']),\n",
        "            'gamma': trial.suggest_categorical('gamma', ['scale', 'auto']),\n",
        "            'probability': True,\n",
        "        }\n",
        "    elif classifier_name == 'RandomForestClassifier':\n",
        "        return {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
        "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20)\n",
        "        }\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown classifier: {classifier_name}\")\n",
        "\n",
        "def get_classifier_instance(classifier_name, params):\n",
        "    if classifier_name == 'Logistic regression':\n",
        "        return LogisticRegression(**params)\n",
        "    elif classifier_name == 'XGBoost':\n",
        "        return XGBClassifier(**params)\n",
        "    elif classifier_name == 'KNeighborsClassifier':\n",
        "        return KNeighborsClassifier(**params)\n",
        "    elif classifier_name == 'DecisionTreeClassifier':\n",
        "        return DecisionTreeClassifier(**params)\n",
        "    elif classifier_name == 'MLPClassifier':\n",
        "        return MLPClassifier(**params)\n",
        "    elif classifier_name == 'SVC':\n",
        "        return SVC(**params)\n",
        "    elif classifier_name == 'RandomForestClassifier':\n",
        "        return RandomForestClassifier(**params)\n",
        "    elif classifier_name == 'GaussianNB':\n",
        "        return GaussianNB(**params)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown classifier: {classifier_name}\")"
      ],
      "metadata": {
        "id": "PYmSIj_tOjA_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "\n",
        "def objective(trial, classifier_name, X_train, y_train):\n",
        "    # Retrieve hyperparameters for the current classifier using the Optuna trial\n",
        "    params = get_classifier_params(trial, classifier_name)\n",
        "\n",
        "    # Instantiate the classifier with the suggested hyperparameters\n",
        "    classifier = get_classifier_instance(classifier_name, params)\n",
        "\n",
        "    # Create a pipeline with StandardScaler and the classifier\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('classifier', classifier)\n",
        "    ])\n",
        "\n",
        "    # Initialize StratifiedKFold for cross-validation\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Evaluate the pipeline using cross_val_score with weighted F1-score\n",
        "    scores = cross_val_score(pipeline, X_train, y_train, cv=kf, scoring='f1_weighted', n_jobs=-1)\n",
        "\n",
        "    # Return the mean F1-score from cross-validation\n",
        "    return scores.mean()\n",
        "\n",
        "print(\"Objective function 'objective' defined successfully.\")"
      ],
      "metadata": {
        "id": "JPsbEGOQOADv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for data_name, (X_train, X_test, y_train, y_test) in train_test_List:\n",
        "    print(f\"\\nProcessing dataset: {data_name}\")\n",
        "    for classifier_name, _ in classifiers:\n",
        "        print(f\"  Optimizing for classifier: {classifier_name}\")\n",
        "\n",
        "        # Create an Optuna study for each classifier and dataset\n",
        "        study = optuna.create_study(direction='maximize')\n",
        "        study.optimize(lambda trial: objective(trial, classifier_name, X_train, y_train), n_trials=10, show_progress_bar=True)\n",
        "\n",
        "        # Store the best results\n",
        "        results.append({\n",
        "            'dataset': data_name,\n",
        "            'classifier': classifier_name,\n",
        "            'best_f1_score': study.best_value,\n",
        "            'best_params': study.best_params\n",
        "        })\n",
        "\n",
        "# Convert results to a Pandas DataFrame for summary\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"best_param.csv\")\n",
        "\n",
        "print(\"Hyperparameter optimization complete. Results stored in 'results_df'.\")"
      ],
      "metadata": {
        "id": "Q8aBbmR9OR3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Experiments"
      ],
      "metadata": {
        "id": "iExAFL2rMx5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "results_df = pd.read_csv(\"best_param.csv\") # getfrom optuna\n",
        "param_dict = results_df.set_index(['dataset', 'classifier'])['best_params'].to_dict()\n",
        "\n",
        "evaluation_results = [] # Re-initialize the evaluation_results list\n",
        "\n",
        "for dataset_name, data_tuple in train_test_List:\n",
        "    X_train, X_test, y_train, y_test = data_tuple\n",
        "    print(\"dataset_name:\", dataset_name)\n",
        "\n",
        "    for oversampler_config in oversampling_methods:\n",
        "      if oversampler_config != None:\n",
        "        oversampler_class_name, oversampler_params = oversampler_config\n",
        "      else:\n",
        "        oversampler_class_name = \"None\"\n",
        "        oversampler_params = {}\n",
        "\n",
        "      print(\"\\t oversampler_config:\", oversampler_config)\n",
        "\n",
        "      for classifier_name, _ in classifiers:\n",
        "        print(\"\\t\\t classifier_name:\", classifier_name)\n",
        "\n",
        "        # Instantiate classifier and create pipeline\n",
        "        best_params_str = param_dict[(dataset_name, classifier_name)]\n",
        "        # Convert the string representation of the dictionary back to a dictionary\n",
        "        best_params = ast.literal_eval(best_params_str)\n",
        "\n",
        "        accuracy_list = []\n",
        "        precision_list = []\n",
        "        recall_list = []\n",
        "        f1_list = []\n",
        "        roc_auc_list = []\n",
        "\n",
        "        # repeat for reproducibility\n",
        "        for _ in range(7):\n",
        "          X_train2, y_train2 = oversampling_only(oversampler_config, X_train, X_test, y_train, y_test)\n",
        "\n",
        "          classifier = get_classifier_instance(classifier_name, best_params)\n",
        "          pipeline = Pipeline(\n",
        "              [('scaler', StandardScaler()), ('classifier', classifier)]\n",
        "          )\n",
        "\n",
        "          # Train the pipeline\n",
        "          pipeline.fit(X_train2, y_train2)\n",
        "\n",
        "          # Make predictions\n",
        "          y_pred = pipeline.predict(X_test)\n",
        "\n",
        "          # Calculate metrics\n",
        "          accuracy = accuracy_score(y_test, y_pred)\n",
        "          precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "          recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "          f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "          roc_auc = np.nan # Initialize as NaN\n",
        "          # Try to get probability predictions for ROC AUC\n",
        "          if hasattr(pipeline.named_steps['classifier'], 'predict_proba'):\n",
        "              y_proba = pipeline.predict_proba(X_test)\n",
        "              unique_classes = np.unique(y_test)\n",
        "\n",
        "              if len(unique_classes) > 1:\n",
        "                  if len(unique_classes) == 2: # Binary classification\n",
        "\n",
        "                      pos_class_label = unique_classes[-1]\n",
        "                      # Get the order of classes learned by the classifier\n",
        "                      class_labels_in_model = pipeline.named_steps['classifier'].classes_\n",
        "                      # Find the index corresponding to the positive class label\n",
        "                      pos_class_idx = np.where(class_labels_in_model == pos_class_label)[0][0]\n",
        "\n",
        "                      y_score_for_roc = y_proba[:, pos_class_idx]\n",
        "\n",
        "                      try:\n",
        "                          # For binary classification, 'multi_class' is not needed, and y_score must be 1D.\n",
        "                          roc_auc = roc_auc_score(y_test, y_score_for_roc, average='weighted')\n",
        "                      except ValueError as e:\n",
        "                          print(f\"Warning: Could not compute ROC AUC for binary case {data_name} with {classifier_name}: {e}\")\n",
        "\n",
        "                  else: # Multi-class classification (len(unique_classes) > 2)\n",
        "                      # For multi-class classification, roc_auc_score expects a 2D array (n_samples, n_classes).\n",
        "                      # The columns of y_proba should correspond to the order of classes in unique_classes.\n",
        "                      if y_proba.ndim == 2 and y_proba.shape[1] == len(unique_classes):\n",
        "                          try:\n",
        "                              roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted', labels=unique_classes)\n",
        "                          except ValueError as e:\n",
        "                              print(f\"Warning: Could not compute ROC AUC for multi-class case {data_name} with {classifier_name}: {e}\")\n",
        "                      else:\n",
        "                          print(f\"Warning: Unexpected y_proba shape for multi-class classification in {data_name} with {classifier_name}. ROC AUC will be NaN.\")\n",
        "              else:\n",
        "                  print(f\"Warning: Cannot compute ROC AUC for {data_name} with classifier {classifier_name} as y_test contains only one class.\")\n",
        "          else:\n",
        "              print(f\"Warning: Classifier {classifier_name} does not support predict_proba. ROC AUC will be NaN.\")\n",
        "\n",
        "          accuracy_list.append(accuracy)\n",
        "          precision_list.append(precision)\n",
        "          recall_list.append(recall)\n",
        "          f1_list.append(f1)\n",
        "          roc_auc_list.append(roc_auc)\n",
        "\n",
        "        # Store results\n",
        "        evaluation_results.append({\n",
        "            'dataset': dataset_name,\n",
        "            'classifier': classifier_name,\n",
        "            'oversamping': oversampler_class_name,\n",
        "            'accuracy': accuracy_list,\n",
        "            'precision': precision_list,\n",
        "            'recall': recall_list,\n",
        "            'f1_score': f1_list,\n",
        "            'roc_auc': roc_auc_list,\n",
        "        })\n",
        "\n",
        "print(\"Model evaluation complete. Results stored in 'evaluation_results'.\")"
      ],
      "metadata": {
        "id": "X5AwW2QBNHCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_df = pd.DataFrame(evaluation_results)\n",
        "print(\"Evaluation DataFrame created successfully:\")\n",
        "display(evaluation_df)\n",
        "evaluation_df.to_csv(\"SONA_results.csv\")"
      ],
      "metadata": {
        "id": "VWwDRY5INY4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}